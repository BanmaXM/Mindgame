# 赢的数据微调数据集说明

## 概述

本数据集是从原始游戏数据中筛选出的"赢的数据"（奖励值 ≥ 0.0），专门用于模型微调。通过使用成功策略的数据进行微调，可以帮助模型学习更有效的博弈策略和对话模式。

## 数据统计

### 原始数据
- 总样本数：2,887个
- 奖励范围：-1.0 到 1.0
- 奖励分布：
  - 奖励 -1.0：1,046个样本 (36.23%)
  - 奖励 0.0：798个样本 (27.64%)
  - 奖励 1.0：1,043个样本 (36.13%)

### 筛选后的赢的数据
- 筛选标准：奖励值 ≥ 0.0
- 赢的样本数：1,841个
- 占比：63.77%
- 训练集：1,471个样本 (80%)
- 验证集：368个样本 (20%)

## 数据格式

数据集提供JSON和CSV两种格式，包含以下字段：

| 字段 | 类型 | 描述 |
|------|------|------|
| instruction | string | 任务指令，描述模型需要执行的任务 |
| input | string | 输入内容，包含游戏历史和上下文 |
| output | string | 期望输出，模型的回应或决策 |
| player_id | int | 玩家ID (0, 1, 或 2) |
| round | int | 游戏轮次 (1-5) |
| phase | string | 游戏阶段 ("chat" 或 "decision") |
| model | string | 使用的模型名称 |
| reward | float | 奖励值 (0.0 或 1.0) |
| agent_type | string | 代理类型 (Agent0, Agent1, 或 Agent2) |
| agent_prompt | string | 代理使用的提示策略 |

## 成功策略分析

### 最成功的Agent Prompt（前5名）
1. advanced_strategy：261个样本 (14.18%)
2. 2_Fair_Retaliator_0：115个样本 (6.25%)
3. 1_Liar_0：109个样本 (5.92%)
4. simple_role_play_2：95个样本 (5.16%)
5. 2_Fair_Retaliator_3：70个样本 (3.81%)

### Agent Type分布
- Agent0：645个样本 (35.07%)
- Agent1：607个样本 (33.01%)
- Agent2：587个样本 (31.92%)

### 最成功的模型（前5名）
1. api/openai-gpt-5：768个样本 (41.76%)
2. api/qwen-qwq-32b：127个样本 (6.91%)
3. api/google-gemini-2.5-pro：111个样本 (6.04%)
4. api/qwen-qwen3-235b-a22b：93个样本 (5.06%)
5. api/openai-gpt-5-nano：83个样本 (4.51%)

## 文件列表

### 完整数据集
- `fine_tuning_data_winning.json`：JSON格式的完整赢的数据集
- `fine_tuning_data_winning.csv`：CSV格式的完整赢的数据集

### 训练集和验证集
- `fine_tuning_data_winning_train.json`：JSON格式的训练集 (1,471个样本)
- `fine_tuning_data_winning_train.csv`：CSV格式的训练集
- `fine_tuning_data_winning_val.json`：JSON格式的验证集 (368个样本)
- `fine_tuning_data_winning_val.csv`：CSV格式的验证集

## 数据特点

1. **高质量策略**：所有数据点都来自成功的游戏策略（奖励 ≥ 0.0）
2. **多样性**：包含不同玩家、不同轮次、不同阶段的数据
3. **策略丰富**：涵盖多种不同的提示策略和代理类型
4. **真实交互**：来自实际游戏交互，而非合成数据

## 微调应用建议

### 适用场景
1. **策略性对话**：训练模型在博弈环境中进行有效的策略性对话
2. **决策制定**：提升模型在复杂环境中的决策能力
3. **角色扮演**：增强模型在不同角色定位下的表现
4. **博弈论应用**：用于开发游戏AI或策略助手

### 微调建议
1. **数据使用**：建议使用训练集进行微调，验证集用于评估模型性能
2. **模型选择**：根据目标应用场景选择合适的基座模型
3. **超参数调整**：根据数据集大小调整学习率和训练轮数
4. **评估指标**：建议使用准确率、F1分数和困惑度等指标评估微调效果

### 注意事项
1. 数据集中的策略可能不适用于所有场景，请根据具体应用需求进行调整
2. 微调后的模型可能在特定任务上表现优秀，但通用能力可能有所下降
3. 建议在微调过程中进行充分的验证和测试

## 数据转换脚本

使用 `analyze_and_filter_winning_data.py` 脚本从原始数据中筛选出赢的数据，该脚本：
1. 分析原始数据的奖励分布
2. 筛选奖励值 ≥ 0.0 的数据
3. 分析成功策略的特点
4. 创建训练集和验证集分割
5. 保存多种格式的数据文件

## 总结

这个赢的数据微调数据集包含了1,841个高质量的游戏交互样本，涵盖了多种成功的策略和决策模式。通过使用这些数据进行微调，可以帮助模型学习更有效的博弈策略和对话技巧，特别适用于需要策略思考和决策能力的应用场景。